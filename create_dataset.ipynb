{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.dataset.cnndm import CNNDMSummarizationDataset\n",
    "from utils_nlp.eval import compute_rouge_python\n",
    "from utils_nlp.models.transformers.extractive_summarization import (\n",
    "    ExtractiveSummarizer,\n",
    "    ExtSumProcessor,\n",
    ")\n",
    "\n",
    "from utils_nlp.models.transformers.datasets import SummarizationDataset\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.set_device(f'cuda:{0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# bertsum train에서 사용한 값과 동일\n",
    "USE_PREPROCSSED_DATA = True\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_POS_LENGTH = 512\n",
    "NUM_GPUS = 1\n",
    "ENCODER = \"transformer\"\n",
    "MAX_STEPS=5e4\n",
    "\n",
    "CACHE_DIR = ''\n",
    "model_save_path = ''\n",
    "processor = ExtSumProcessor(model_name=MODEL_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading a previous saved model\n",
    "import torch\n",
    "model_path = os.path.join(\n",
    "        model_save_path,\n",
    "        \"extsum_modelname_{0}_usepreprocess{1}_steps_{2}.pt\".format(\n",
    "            MODEL_NAME, USE_PREPROCSSED_DATA, MAX_STEPS\n",
    "        ))\n",
    "summarizer = ExtractiveSummarizer(processor, MODEL_NAME, ENCODER, MAX_POS_LENGTH, CACHE_DIR)\n",
    "summarizer.model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XSUM PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "xsum_train = load_dataset('xsum', split='train')\n",
    "xsum_test = load_dataset('xsum', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = []\n",
    "train_tgt = []\n",
    "\n",
    "for row in xsum_train:\n",
    "    if len(row['document'])==0 or len(row['summary'])==0:\n",
    "        continue\n",
    "    train_src.append(row['document'])\n",
    "    train_tgt.append(row['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SummarizationDataset(\n",
    "    None,\n",
    "    source=train_src,\n",
    "    source_preprocessing=[tokenize.sent_tokenize],\n",
    "    target=train_tgt,\n",
    "    word_tokenize=nltk.word_tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ExtSumProcessor(model_name=MODEL_NAME,  cache_dir=CACHE_DIR)\n",
    "preprocessed_traindata = processor.preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n : salient sentences ratio (value: 0~1)\n",
    "prediction = summarizer.predict(preprocessed_traindata, num_gpus=1, batch_size=10, sentence_separator=\"\\n\", top_n=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token = []\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    src_txt = preprocessed_traindata[i]['src_txt']\n",
    "    exts = prediction[i].split('\\n')\n",
    "    mask = []\n",
    "    for sent in src_txt:\n",
    "        if sent in exts:\n",
    "            mask.append(True)\n",
    "        else: mask.append(False)\n",
    "    document = [0]\n",
    "    pos = []\n",
    "    nos = []\n",
    "    for j, sent in enumerate(src_txt):\n",
    "        sent = sent + '\\n'\n",
    "        tokens = tokenizer.encode(sent, add_special_tokens=False, truncation=True)\n",
    "        if mask[j]:\n",
    "            pos.append([len(document), len(document)+ len(tokens) -1])\n",
    "        else: nos.append([len(document), len(document) + len(tokens) -1 ])\n",
    "        document.extend(tokens)\n",
    "    document[-1] = 2\n",
    "    summary = tokenizer(preprocessed_traindata[i]['tgt_txt'] ,add_special_tokens=True, truncation=True).input_ids\n",
    "    dic = {'document':document, 'summary':summary, 'pos':pos, 'nos':nos}\n",
    "    \n",
    "    train_token.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path = 'train'\n",
    "with open(train_save_path, 'wb') as f:\n",
    "    pickle.dump(train_token, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = []\n",
    "\n",
    "for row in xsum_test:\n",
    "    document = tokenizer(row['document'], add_special_tokens=True, truncation=True).input_ids\n",
    "    summary = tokenizer(row['summary'], add_special_tokens=True, truncation=True).input_ids\n",
    "    if len(document)==0 or len(summary)==0: continue\n",
    "    dic = {'docuemnt':document, 'summary':summary}\n",
    "    test_token.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_save_path = 'test'\n",
    "with open(test_save_path, 'wb') as f:\n",
    "    pickle.dump(test_token, f)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "84373d2f85ca3329869644c67b1c4ca0d662b41363743a8672a413b8c5b866f8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bertsum': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
